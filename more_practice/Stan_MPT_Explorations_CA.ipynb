{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650025f-240d-41c4-a803-c57dc10f0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import stan\n",
    "import arviz as az\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde58b8-99fe-4728-a411-25c394dfc8e7",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will examine a simulated task focused on **word production**. The real task involves a study of individuals with **aphasia** — a language disorder often resulting from a stroke or head injury. \n",
    "\n",
    "In our simulated task, participants are shown an image, such as that of a cat, and asked to identify it. Responses can vary, for instance, a participant might:\n",
    "\n",
    "1. correctly identify the image as a *cat*,\n",
    "2. produce a non-word that has a phonological relation to the target (i.e., a **neologism**), such as *cag*,\n",
    "3. provide a semantically unrelated, yet phonologically similar word like *hat* (i.e., a **formal** mistake),\n",
    "4. offer a semantically and phonologically similar but incorrect identification such as *rat* (i.e., a **mixed** mistake), or\n",
    "5. omit the response altogether or provide a response that does not fit the above categories (e.g., a lengthy description)\n",
    "\n",
    "Below, we we build a theoretical multiprocessing tree model (MPT) to explain the frequencies of these outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b7d27-f44e-4ea5-8125-6f9d2a30256c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>neologism</th>\n",
       "      <th>formal</th>\n",
       "      <th>mixed</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>responses</th>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           correct  neologism  formal  mixed  other\n",
       "responses       47         12      19      4     18"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a local RNG with a fixed seed\n",
    "global_rng = np.random.default_rng(42)\n",
    "\n",
    "# Fix true probabilities for simulation study\n",
    "true_probs = {\n",
    "    'correct': .42,\n",
    "    'neologism': .1,\n",
    "    'formal': .2,\n",
    "    'mixed': .08,\n",
    "    'other': .2 # 1 - rest\n",
    "}\n",
    "\n",
    "\n",
    "# Simulate data\n",
    "num_trials = 100\n",
    "y = global_rng.multinomial(n=num_trials, pvals=list(true_probs.values()))\n",
    "pd.DataFrame(y[:, None].T, columns=true_probs.keys(), index=['responses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea8a85d-0997-4c1e-8192-285ed4965c75",
   "metadata": {},
   "source": [
    "## Modeling Multiple Responses via a Multinomial Likelihood\n",
    "\n",
    "We will first build a baseline model that will estimate the underlying (latent) probabilities from the observed frequency data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f73b7-a13c-4e6e-8905-35282e416f1c",
   "metadata": {},
   "source": [
    "### Stan Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1774ce88-0c1a-464e-b227-bdbd6d10e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_program_code = \"\"\"\n",
    "data {\n",
    "  int<lower=1> N; // Number of trials\n",
    "  int<lower=1> K; // Number of categories\n",
    "  array[K] int<lower=0, upper=N> freqs;\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  // Our code here\n",
    "}\n",
    "\n",
    "model {\n",
    " // Our code here\n",
    "}\n",
    "\n",
    "generated quantities{\n",
    "  // Obtaining predictions\n",
    "  array[K] int preds = multinomial_rng(theta, N);\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf001418-229d-43a8-8f06-9b2a5d03db17",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313853e-2f62-4636-83b8-2c777564f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "stan_dict = {\n",
    "    'freqs': y,\n",
    "    'N': num_trials,\n",
    "    'K': y.shape[0]\n",
    "}\n",
    "\n",
    "# Compile model\n",
    "posterior = stan.build(baseline_program_code, data=stan_dict, random_seed=42)\n",
    "\n",
    "\n",
    "# Sample (i.e., inverse inference)\n",
    "fit = posterior.sample(num_chains=4, num_samples=2500, num_warmup=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3da94-5f6b-4ee1-9602-a31b525333d7",
   "metadata": {},
   "source": [
    "### Model Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afbc21-08e2-4d4e-8dfc-10317ad70ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation summary, convergence, and efficiency diagnostics\n",
    "az.summary(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix true probabilities for simulation study\n",
    "true_probs = {\n",
    "    'correct': .42,\n",
    "    'neologism': .1,\n",
    "    'formal': .2,\n",
    "    'mixed': .08,\n",
    "    'other': .2 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc3261-d848-44f6-8b09-4041e1f4e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traceplots and marginals - visual convergence checks\n",
    "axarr = az.plot_trace(fit)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect all outputs\n",
    "fit.to_frame().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034ff65-3c51-49c0-88be-39f65e97d4c4",
   "metadata": {},
   "source": [
    "## Modeling Multiple Responses via a Multinomial Processing Tree (MPT) Model.\n",
    "\n",
    "We will now build a process model that will estimate the underlying (latent) **process probabilities** from the observed frequency data.\n",
    "\n",
    "Walker, Hickok, and Fridriksson (2018) created an MPT model that specifies a set of possible internal errors that lead to the various possible response types during a picture naming trial for aphasic patients. Here we’ll explore a simplification of the original model.\n",
    "\n",
    "Our model assumes that when an attempt is made to produce a word, errors in production can arise at the whole word level (lexical level) or the morpheme level (phonological level). Semantic errors are assumed to arise from the lexical substitutions, and neologism errors from phonological substitutions. Real word responses that are phonologically related to the correct target word can arise from substitutions at the lexical or phonological level.\n",
    "\n",
    "1. Walker, G. M., Hickok, G., & Fridriksson, J. (2018). A cognitive psychometric model for assessment of picture naming abilities in aphasia. *Psychological Assessment, 30*(6), 809. http://dx.doi.org/10.1037/pas0000529 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591e64b-dba3-44e3-a50e-b0682bfe4546",
   "metadata": {},
   "source": [
    "### Parameters and Model\n",
    "\n",
    "The model is depicted in the following figure:\n",
    "\n",
    "<img src=\"mpt_fig.png\" width=65% height=65% />\n",
    "\n",
    "\n",
    "The table below lists the four latent parameters and corresponding interpretations:\n",
    "\n",
    "\n",
    "| Parameter | Description | \n",
    "| --- | --- |\n",
    "| $a$ | Probability of initiating an attempt |\n",
    "| $t$ | Probability of selecting a target word from a pool of candidates |\n",
    "| $f$ | Probability of retrieving correct phonemes |\n",
    "| $c$ | Probability of a phoneme change in the target word |\n",
    "\n",
    "The table below lists the five types of responses (categories):\n",
    "\n",
    "\n",
    "| Response (Category) | Description | Example | \n",
    "| --- | --- | --- |\n",
    "| Correct | The response matches the target image. | cat |\n",
    "| Neologism | The response is not a word, but it has a phonological relation to the target image. | cag |\n",
    "| Formal | The response is a word with only a phonological relation to the target image. | hat |\n",
    "| Mixed | The response is a word with both a semantic and phonological relation the target image. | rat |\n",
    "| Other | All other responses, including omissions, descriptions, non-nouns, etc. | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e26ec-cdfe-4494-8720-33666b5fecc1",
   "metadata": {},
   "source": [
    "### Simulating the Model\n",
    "\n",
    "Practice: We first need to derive the equations for the category probabilities according to the graphical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11806136-6d95-476b-8bbd-52d7a56aa799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix true parameters\n",
    "true_params = {\n",
    "    'a': 0.75,\n",
    "    't': 0.9,\n",
    "    'f': 0.8,\n",
    "    'c': .2\n",
    "}\n",
    "a = true_params[\"a\"]\n",
    "t = true_params[\"t\"]\n",
    "f = true_params[\"f\"]\n",
    "c = true_params[\"c\"]\n",
    "\n",
    "# Fix true probabilities and simulate from model\n",
    "num_trials = 120\n",
    "true_probs = {\n",
    "    'correct': a * t * f,\n",
    "    'neologism': a * (1 - t) * (1 - f) * (1 - c) + a * t * (1 - f) * (1 - c),\n",
    "    'formal': a * (1 - t) * (1 - f) * c + a * t * (1 - f) * c,\n",
    "    'mixed': a * (1 - t) * f,\n",
    "    'other': 1 - a\n",
    "}\n",
    "\n",
    "data = global_rng.multinomial(n=num_trials, pvals=list(true_probs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c48a96-356d-44a0-a2f4-f43f8f8dfd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpt_model_code = \"\"\"\n",
    "data {\n",
    "  int<lower=1> N; // Number of trials\n",
    "  int<lower=1> K; // Number of categories\n",
    "  array[K] int<lower=0, upper=N> freqs;\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  // Your code here\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "  simplex[K] theta;\n",
    "  // Your code here - need to fill in the entries of the simplex according to the tree\n",
    "}\n",
    "\n",
    "model {\n",
    "  // Assuming uniform priors\n",
    "  target += beta_lpdf(a | 1, 1);\n",
    "  target += beta_lpdf(t | 1, 1);\n",
    "  target += beta_lpdf(f | 1, 1);\n",
    "  target += beta_lpdf(c | 1, 1);\n",
    "\n",
    "  // Multinomial likelihood\n",
    "  target += multinomial_lpmf(freqs | theta);\n",
    "}\n",
    "\n",
    "generated quantities{\n",
    "  array[K] int pred_freqs;\n",
    "  pred_freqs = multinomial_rng(theta, N);\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905735e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile, fit, and diagnose model\n",
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48e7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter recovery and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06f5a2",
   "metadata": {},
   "source": [
    "## Global Parameter Recovery Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42371b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Our code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cbd1a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognitive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
